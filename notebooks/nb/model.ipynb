{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading tfrecords & Parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T05:13:10.423855Z",
     "start_time": "2018-07-18T05:13:09.576850Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T05:13:15.634921Z",
     "start_time": "2018-07-18T05:13:15.551945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parser for TFRecord\n",
    "def parser(serialized_example):\n",
    "    \n",
    "    features = {\n",
    "        'label': tf.FixedLenFeature([], tf.float32),\n",
    "        'image': tf.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    \n",
    "    parsed_feature = tf.parse_single_example(serialized_example, features)\n",
    "    \n",
    "    weight = parsed_feature['label']\n",
    "    \n",
    "    img = parsed_feature['image']\n",
    "    img = tf.string_to_number(img, tf.float32)\n",
    "    \n",
    "    return weight, img\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train =  \"../sample_image/preprocessed/train.tfrecord\"\n",
    "test = \"../sample_image/preprocessed/test.tfrecord\"\n",
    "\n",
    "# Dataset\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "# Dataset \n",
    "train_dataset = tf.data.TFRecordDataset(train).map(parser)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(777)\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(test).map(parser)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).shuffle(777)\n",
    "\n",
    "itr = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "weight, img = itr.get_next()\n",
    "\n",
    "img = tf.reshape(img, [-1, 250, 550, 3]) # 차원 주의 !\n",
    "# img = tf.cast(img, tf.float32)\n",
    "\n",
    "weight = tf.reshape(weight, [-1]) # 차원 주의 ! \n",
    "\n",
    "train_init_op = itr.make_initializer(train_dataset)\n",
    "test_init_op = itr.make_initializer(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T05:13:38.093371Z",
     "start_time": "2018-07-18T05:13:37.729372Z"
    }
   },
   "outputs": [],
   "source": [
    "def model(x, activation, dropout_prob, reuse=False):\n",
    "    \n",
    "    # first layer for gray scale \n",
    "    gray_conv = tf.layers.conv2d(img, filters=1, kernel_size=1, \n",
    "                             padding='valid', activation=activation, \n",
    "                             reuse=reuse, name='gray_conv')\n",
    "    ##### VGG alike #####\n",
    "    conv1 = tf.layers.conv2d(gray_conv, filters=16, kernel_size=7, \n",
    "                             padding='valid', activation=activation, \n",
    "                             reuse=reuse, name='conv1')\n",
    "    conv1_1 = tf.layers.conv2d(conv1, filters=16, kernel_size=7, \n",
    "                             padding='valid', activation=activation, \n",
    "                             reuse=reuse, name='conv1_1')\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(conv1_1, pool_size=2, strides=2)\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(pool1, filters=16, kernel_size=7,\n",
    "                            padding=\"valid\", activation=activation,\n",
    "                            reuse=reuse, name='conv2')\n",
    "    conv2_2 = tf.layers.conv2d(conv2, filters=16, kernel_size=7,\n",
    "                            padding=\"valid\", activation=activation,\n",
    "                            reuse=reuse, name='conv2_2')\n",
    "    pool2 = tf.layers.max_pooling2d(conv2_2, pool_size=2, strides=2)\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(pool2, filters=32, kernel_size=3, \n",
    "                            padding=\"SAME\", activation=activation,\n",
    "                            reuse=reuse, name='conv3')\n",
    "    conv3_3 = tf.layers.conv2d(conv3, filters=32, kernel_size=3, \n",
    "                            padding=\"SAME\", activation=activation,\n",
    "                            reuse=reuse, name='conv3_3')\n",
    "    pool3 = tf.layers.max_pooling2d(conv3_3, pool_size=3, strides=2)\n",
    "\n",
    "    ##############################################################\n",
    "   \n",
    "    flat = tf.layers.flatten(pool3)\n",
    "    \n",
    "    dropout1 = tf.layers.dropout(flat, rate=dropout_prob)\n",
    "    fc1 = tf.layers.dense(dropout1, units=100, reuse=reuse, \n",
    "                          name='fc1')\n",
    "\n",
    "    dropout2 = tf.layers.dropout(fc1, rate=dropout_prob)\n",
    "    out = tf.layers.dense(dropout2, 1, reuse=reuse, name='out')\n",
    "    \n",
    "    return out \n",
    "\n",
    "train_out = model(img, tf.nn.relu, 0.7)\n",
    "train_out = tf.reshape(train_out, [-1])\n",
    "test_out = model(img, tf.nn.relu, 1, True)\n",
    "test_out = tf.reshape(test_out, [-1])\n",
    "\n",
    "loss = tf.losses.mean_squared_error(weight, train_out)\n",
    "train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "summary = tf.summary.scalar('train_loss', loss)\n",
    "\n",
    "# test(validation) score\n",
    "pred = test_out\n",
    "mse = tf.metrics.mean_squared_error(weight, pred)\n",
    "    \n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-18T05:09:45.862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start {}\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "#     writer = tf.summary.FileWriter('../logs/', sess.graph)\n",
    "    loss_graph = []\n",
    "    mse_graph = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        sess.run(train_init_op)\n",
    "        for train_step in range(9999999):\n",
    "            try :\n",
    "                print(\"start {}\")\n",
    "                _, _loss, _summ = sess.run([train_op, loss, summary])\n",
    "                print(_loss)\n",
    "                loss_graph.append(_loss)\n",
    "                writer.add_summary(_summ, train_step)\n",
    "                    \n",
    "            except tf.errors.OutOfRangeError :\n",
    "                break\n",
    "                \n",
    "                \n",
    "        sess.run(test_init_op)\n",
    "        for test_step in range(99999999):\n",
    "                try:\n",
    "                    _mse = sess.run(mse)\n",
    "                    mse_graph.append(_mse)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        print('epochs : {}, train_loss : {}, val_mse : {}'.format(epoch, _loss, _mse[0]))\n",
    "        saver.save(sess, \"../logs/cnn{}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "#     writer = tf.summary.FileWriter('../logs/', sess.graph)\n",
    "    loss_graph = []\n",
    "    mse_graph = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        sess.run(train_init_op)\n",
    "        for train_step in range(9999999):\n",
    "            try :\n",
    "                _, _loss, _summ = sess.run([train_op, loss, summary])\n",
    "                loss_graph.append(_loss)\n",
    "                writer.add_summary(_summ, train_step)\n",
    "                    \n",
    "            except tf.errors.OutOfRangeError :\n",
    "                print(error)\n",
    "                break\n",
    "                \n",
    "                \n",
    "        sess.run(test_init_op)\n",
    "        for test_step in range(99999999):\n",
    "                try:\n",
    "                    _mse = sess.run(mse)\n",
    "                    mse_graph.append(_mse)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        print('epochs : {}, train_loss : {}, val_mse : {}'.format(epoch, _loss, _mse[0]))\n",
    "        saver.save(sess, \"../logs/cnn{}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Implementation \n",
    "\n",
    "> - link(https://github.com/YixuanLi/densenet-tensorflow/blob/master/cifar10-densenet.py#L25)\n",
    "> - PAPER(https://arxiv.org/pdf/1608.06993.pdf)\n",
    "> - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DenseNet:\n",
    "    def batch_norm_relu(self, inputs, is_training, reuse, name):\n",
    "        bn = tf.layers.batch_normalization(inputs, \n",
    "                                           training=is_training, \n",
    "                                           reuse=reuse, \n",
    "                                           name=name)\n",
    "        outputs = tf.nn.relu(bn)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def initial_conv(self, inputs, reuse=False):\n",
    "        l = tf.layers.conv2d(inputs=inputs,\n",
    "                             filters=16,\n",
    "                             kernel_size=3,\n",
    "                             strides=2,\n",
    "                             padding='SAME',\n",
    "                             name='init_conv',\n",
    "                             reuse=reuse)\n",
    "        \n",
    "        return l\n",
    "\n",
    "    \n",
    "    def composite_layer(self, inputs, keep_prob, name, is_training=True, reuse=False):\n",
    "        l = inputs\n",
    "        l = self.batch_norm_relu(l, is_training, reuse, name=name+'_bn1')\n",
    "        l = tf.layers.conv2d(l, 4 * 12, 1, 1, \n",
    "                             padding='SAME', name=name+'_conv1', reuse=reuse)\n",
    "            \n",
    "        l = self.batch_norm_relu(l, is_training, reuse, name=name+'_bn2')\n",
    "        \n",
    "        l = tf.layers.conv2d(l, 12, 3, 1, \n",
    "                             padding='SAME', name=name+'_conv2', reuse=reuse)\n",
    "        \n",
    "        l = tf.layers.dropout(l, keep_prob, training=is_training)\n",
    "        \n",
    "        return tf.concat([inputs, l], axis=3) \n",
    "\n",
    "\n",
    "    def transition_layer(self, inputs, name, is_training=True, reuse=False):\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        n_filters = int(shape[3] * 0.5)\n",
    "        \n",
    "        l = self.batch_norm_relu(inputs, is_training, reuse, name=name + '_bn')\n",
    "        l = tf.layers.conv2d(l, n_filters, 1, 1, padding='SAME', name=name + '_conv', reuse=reuse)\n",
    "        l = tf.layers.average_pooling2d(l, 2, 2, name='pool')\n",
    "\n",
    "        return l\n",
    "    \n",
    "    def dense_net(self, inputs, keep_prob=0.2, is_training=True, reuse=False):\n",
    "        l = self.initial_conv(inputs=inputs, reuse=reuse)\n",
    "        \n",
    "        with tf.variable_scope('block1') as scope:\n",
    "            for i in range(6):\n",
    "                l = self.composite_layer(l, \n",
    "                                         keep_prob, \n",
    "                                         name='dense_layer{}'.format(i), \n",
    "                                         is_training=is_training,\n",
    "                                         reuse=reuse)\n",
    "                \n",
    "            l = self.transition_layer(l, \n",
    "                                     name='transition1',\n",
    "                                     is_training=is_training,\n",
    "                                     reuse=reuse)\n",
    "                \n",
    "        with tf.variable_scope('block2') as scope:\n",
    "            for i in range(12):\n",
    "                l = self.composite_layer(l, \n",
    "                                         keep_prob, \n",
    "                                         name='dense_layer{}'.format(i),\n",
    "                                         is_training=is_training,\n",
    "                                         reuse=reuse)\n",
    "\n",
    "            l = self.transition_layer(l, \n",
    "                                     name='transition2',\n",
    "                                     is_training=is_training,\n",
    "                                     reuse=reuse)\n",
    "\n",
    "        with tf.variable_scope('block3') as scope:\n",
    "            for i in range(24):\n",
    "                l = self.composite_layer(l, \n",
    "                                         keep_prob, \n",
    "                                         name='dense_layer{}'.format(i),\n",
    "                                         is_training=is_training,\n",
    "                                         reuse=reuse)\n",
    "                \n",
    "            l = self.transition_layer(l, \n",
    "                                     name='transition3',\n",
    "                                     is_training=is_training,\n",
    "                                     reuse=reuse)\n",
    "\n",
    "        with tf.variable_scope('block4') as scope:\n",
    "            for i in range(16):\n",
    "                l = self.composite_layer(l, \n",
    "                                         keep_prob, \n",
    "                                         name='dense_layer{}'.format(i),\n",
    "                                         is_training=is_training,\n",
    "                                         reuse=reuse)\n",
    "                \n",
    "        return l\n",
    "    \n",
    "    \n",
    "    def get_logits(self, inputs, is_training=True, reuse=False):\n",
    "        l = self.dense_net(inputs, keep_prob=0.2, is_training=is_training, reuse=reuse)\n",
    "        \n",
    "        outputs = self.batch_norm_relu(l, is_training, reuse, name='last_bn')\n",
    "\n",
    "        shape = outputs.get_shape().as_list()\n",
    "        \n",
    "        pool_size = (shape[1], shape[2])\n",
    "        outputs= tf.layers.average_pooling2d(outputs, pool_size=pool_size, strides=1, padding='VALID')\n",
    "        \n",
    "        outputs = tf.layers.flatten(outputs)\n",
    "        outputs = tf.layers.dense(outputs, 12, name='final_dense', reuse=reuse)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "# class CnnLstm:\n",
    "#     def __init__(self):\n",
    "#         self.num_classes = 12\n",
    "#         self.num_filters = [8, 16, 32, 32]\n",
    "#         self.filter_sizes = [7, 3, 3, 3]\n",
    "#         self.pool_sizes = [2, 2, 1, 1]\n",
    "#         self.cnn_dropout_keep_prob = [0, 0.3, 0.4, 0.4]\n",
    "#         self.fc_hidden_units = [1028, 512, 256]\n",
    "#         self.fc_dropout_keep_prob = [0.2, 0.3, 0.35]\n",
    "#         self.lstm_n_hiddens = [512]\n",
    "#         self.lstm_dropout_keep_prob = [0.5]\n",
    "#         self.idx_convolutional_layers = range(1, len(self.filter_sizes) + 1)\n",
    "#         self.idx_fc_layers = range(1, len(self.fc_hidden_units) + 1)\n",
    "#         self.idx_lstm_layers = range(1, len(self.lstm_n_hiddens) + 1)\n",
    "        \n",
    "        \n",
    "#     def convolutional_layer(self, inputs, is_training=True, reuse=False):\n",
    "#         l = inputs\n",
    "        \n",
    "#         for i, num_filter, filter_size, pool_size, keep_prob in zip(self.idx_convolutional_layers,\n",
    "#                                                                     self.num_filters,\n",
    "#                                                                     self.filter_sizes,\n",
    "#                                                                     self.pool_sizes,\n",
    "#                                                                     self.cnn_dropout_keep_prob):\n",
    "#             l = tf.layers.conv2d(l, \n",
    "#                                  filters=num_filter, \n",
    "#                                  kernel_size=filter_size, \n",
    "#                                  strides=1, \n",
    "#                                  padding=\"SAME\", \n",
    "#                                  name=\"conv\"+str(i),\n",
    "#                                  reuse=reuse)\n",
    "            \n",
    "#             l = tf.layers.batch_normalization(l, training=is_training, name=\"conv_bn\"+str(i), reuse=reuse)\n",
    "#             l = tf.nn.relu(l, name=\"conv_relu\"+str(i))\n",
    "#             l = tf.layers.dropout(l, rate=keep_prob, training=is_training, name=\"conv_dropout\"+str(i))\n",
    "\n",
    "#             if pool_size != 1:\n",
    "#                 l = tf.layers.max_pooling2d(l, pool_size=pool_size, strides=pool_size, padding=\"SAME\")\n",
    "                \n",
    "#         return l\n",
    "        \n",
    "    \n",
    "    def fc_layer(self, inputs, is_training=True, reuse=False):\n",
    "        l = inputs\n",
    "        \n",
    "        for i, units, keep_prob in zip(self.idx_fc_layers, self.fc_hidden_units, self.fc_dropout_keep_prob):\n",
    "            l = tf.layers.dense(inputs, units=units, reuse=reuse, name=\"fc\"+str(i))\n",
    "            l = tf.layers.batch_normalization(l, training=is_training, name=\"fc_bn\"+str(i), reuse=reuse)\n",
    "            l = tf.nn.relu(l, name=\"fc_relu\"+str(i))\n",
    "            l = tf.layers.dropout(l, rate=keep_prob, training=is_training, name=\"fc_dropout\"+str(i))\n",
    "            \n",
    "        return l\n",
    "  \n",
    "\n",
    "    def lstm_layer(self, inputs, is_training=True, reuse=False):\n",
    "        if is_training:\n",
    "            keep_probs = [0.5]\n",
    "            \n",
    "        else:\n",
    "            keep_probs = [1]\n",
    "            \n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(512, reuse=reuse)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probs[0])\n",
    "        \n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        \n",
    "        return outputs\n",
    " \n",
    "\n",
    "    def get_reshaped_cnn_to_rnn(self, inputs):\n",
    "        shape = inputs.get_shape().as_list() \n",
    "        inputs = tf.transpose(inputs, [0, 2, 1, 3])\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, shape[2], shape[1] * shape[3]])\n",
    "        \n",
    "        return reshaped_inputs\n",
    "  \n",
    "\n",
    "    def get_logits(self, inputs, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"conv_layers\") as scope:\n",
    "            l = inputs\n",
    "            l = self.convolutional_layer(l, is_training, reuse)\n",
    "            \n",
    "        with tf.variable_scope(\"lstm_layers\") as scope:\n",
    "            reshaped_l = self.get_reshaped_cnn_to_rnn(l)\n",
    "            \n",
    "            l = self.lstm_layer(reshaped_l, is_training, reuse)\n",
    "            \n",
    "        with tf.variable_scope(\"fc_layers\") as scope:\n",
    "            l = tf.layers.flatten(l)\n",
    "            l = self.fc_layer(l, is_training, reuse)\n",
    "                \n",
    "        output = tf.layers.dense(l, units=self.num_classes, reuse=reuse, name='out')\n",
    "            \n",
    "        return output\n",
    "    \n",
    "\n",
    "def train_parser(serialized_example):\n",
    "    features = {\n",
    "        \"spectrum\": tf.FixedLenFeature([12800], tf.float32),\n",
    "        \"label\": tf.FixedLenFeature([12], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed_feature = tf.parse_single_example(serialized_example, features)\n",
    "\n",
    "    spec = parsed_feature['spectrum']\n",
    "    label = parsed_feature['label']\n",
    "\n",
    "    return spec, label\n",
    "        \n",
    "    \n",
    "def test_parser(serialized_example):\n",
    "    features = {\n",
    "        \"spectrum\": tf.FixedLenFeature([12800], tf.float32),\n",
    "    }\n",
    "\n",
    "    parsed_feature = tf.parse_single_example(serialized_example, features)\n",
    "\n",
    "    spec = parsed_feature['spectrum']\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T02:36:12.629886Z",
     "start_time": "2018-07-17T02:36:08.166170Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MAC/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc893819a4ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglobal_avg_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxavier_initializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marg_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tflearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_training_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tflearn/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tflearn/variables.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_arg_scope\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontrib_add_arg_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/contrib/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftplus_inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtridiag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf_normal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compute_weighted_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_RegressionHead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/contrib/learn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/contrib/learn/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/basic_session_run_hooks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m'tf.contrib.learn.basic_session_run_hooks.LoggingTensorHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m'tf.train.LoggingTensorHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     basic_session_run_hooks.LoggingTensorHook)\n\u001b[0m\u001b[1;32m     34\u001b[0m StopAtStepHook = deprecated_alias(\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m'tf.contrib.learn.basic_session_run_hooks.StopAtStepHook'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mdeprecated_alias\u001b[0;34m(deprecated_name, name, func_or_class, warn_once)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# Make a new class with __init__ wrapped in a warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mclass\u001b[0m \u001b[0mNewClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_or_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m       __doc__ = decorator_utils.add_notice_to_docstring(\n\u001b[1;32m    149\u001b[0m           \u001b[0mfunc_or_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Please use %s instead.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mNewClass\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m                            'It will be removed in a future version. '])\n\u001b[1;32m    153\u001b[0m       \u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_or_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       \u001b[0m__module__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36m_call_location\u001b[0;34m(outer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0;34m\"\"\"Returns call location given level up from current call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m   \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# CPython internals are available, use them for performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mcurrentframe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;34m\"\"\"TFDecorator-aware replacement for inspect.currentframe.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[0;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0mframeinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1445\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    766\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Invalidate cache if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# only return a non-existent filename if the module has a PEP 302 loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__loader__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;31m# or it is in the linecache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;31m# Copy sys.modules in order to cope with changes while iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_filesbymodname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tflearn.layers.conv import global_avg_pool \n",
    "from tensorflow.contrib.layers import batch_norm, flatten\n",
    "from tensorflow.contrib.layers import xavier_initializer # weight \n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from cifar10 import *\n",
    "\n",
    "# Hyperparameter\n",
    "growth_k = 24 # growth = \n",
    "nb_block = 2 # how many (dense block + Transition Layer) ?\n",
    "init_learning_rate = 1e-4\n",
    "epsilon = 1e-4 # AdamOptimizer epsilon\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Momentum Optimizer will use\n",
    "nesterov_momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Label & batch_size\n",
    "batch_size = 64\n",
    "\n",
    "iteration = 782\n",
    "# batch_size * iteration = data_set_number\n",
    "\n",
    "test_iteration = 10\n",
    "\n",
    "total_epochs = 300\n",
    "\n",
    "def conv_layer(input, filter, kernel, stride=1, layer_name=\"conv\"):\n",
    "    with tf.name_scope(layer_name):\n",
    "        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding='SAME')\n",
    "        return network\n",
    "\n",
    "def Global_Average_Pooling(x, stride=1):\n",
    "    \"\"\"\n",
    "    width = np.shape(x)[1]\n",
    "    height = np.shape(x)[2]\n",
    "    pool_size = [width, height]\n",
    "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride) # The stride value does not matter\n",
    "    It is global average pooling without tflearn\n",
    "    \"\"\"\n",
    "\n",
    "    return global_avg_pool(x, name='Global_avg_pooling')\n",
    "    # But maybe you need to install h5py and curses or not\n",
    "\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "\n",
    "def Drop_out(x, rate, training) :\n",
    "    return tf.layers.dropout(inputs=x, rate=rate, training=training)\n",
    "\n",
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Average_pooling(x, pool_size=[2,2], stride=2, padding='VALID'):\n",
    "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
    "\n",
    "\n",
    "def Max_Pooling(x, pool_size=[3,3], stride=2, padding='VALID'):\n",
    "    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
    "\n",
    "def Concatenation(layers) :\n",
    "    return tf.concat(layers, axis=3)\n",
    "\n",
    "def Linear(x) :\n",
    "    return tf.layers.dense(inputs=x, units=class_num, name='linear')\n",
    "\n",
    "def Evaluate(sess):\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_pre_index = 0\n",
    "    add = 1000\n",
    "\n",
    "    for it in range(test_iteration):\n",
    "        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n",
    "        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n",
    "        test_pre_index = test_pre_index + add\n",
    "\n",
    "        test_feed_dict = {\n",
    "            x: test_batch_x,\n",
    "            label: test_batch_y,\n",
    "            learning_rate: epoch_learning_rate,\n",
    "            training_flag: False\n",
    "        }\n",
    "\n",
    "        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n",
    "\n",
    "        test_loss += loss_ / 10.0\n",
    "        test_acc += acc_ / 10.0\n",
    "\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag='test_loss', simple_value=test_loss),\n",
    "                                tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
    "\n",
    "    return test_acc, test_loss, summary\n",
    "\n",
    "class DenseNet():\n",
    "    def __init__(self, x, nb_blocks, filters, training):\n",
    "        self.nb_blocks = nb_blocks\n",
    "        self.filters = filters\n",
    "        self.training = training\n",
    "        self.model = self.Dense_net(x)\n",
    "\n",
    "\n",
    "    def bottleneck_layer(self, x, scope):\n",
    "        # print(x)\n",
    "        with tf.name_scope(scope):\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "            x = conv_layer(x, filter=4 * self.filters, kernel=[1,1], layer_name=scope+'_conv1')\n",
    "            x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
    "\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch2')\n",
    "            x = Relu(x)\n",
    "            x = conv_layer(x, filter=self.filters, kernel=[3,3], layer_name=scope+'_conv2')\n",
    "            x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
    "\n",
    "            # print(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def transition_layer(self, x, scope):\n",
    "        with tf.name_scope(scope):\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "            x = conv_layer(x, filter=self.filters, kernel=[1,1], layer_name=scope+'_conv1')\n",
    "            x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
    "            x = Average_pooling(x, pool_size=[2,2], stride=2)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def dense_block(self, input_x, nb_layers, layer_name):\n",
    "        with tf.name_scope(layer_name):\n",
    "            layers_concat = list()\n",
    "            layers_concat.append(input_x)\n",
    "\n",
    "            x = self.bottleneck_layer(input_x, scope=layer_name + '_bottleN_' + str(0))\n",
    "\n",
    "            layers_concat.append(x)\n",
    "\n",
    "            for i in range(nb_layers - 1):\n",
    "                x = Concatenation(layers_concat)\n",
    "                x = self.bottleneck_layer(x, scope=layer_name + '_bottleN_' + str(i + 1))\n",
    "                layers_concat.append(x)\n",
    "\n",
    "            x = Concatenation(layers_concat)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def Dense_net(self, input_x):\n",
    "        x = conv_layer(input_x, filter=2 * self.filters, kernel=[7,7], stride=2, layer_name='conv0')\n",
    "        # x = Max_Pooling(x, pool_size=[3,3], stride=2)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(self.nb_blocks) :\n",
    "            # 6 -> 12 -> 48\n",
    "            x = self.dense_block(input_x=x, nb_layers=4, layer_name='dense_'+str(i))\n",
    "            x = self.transition_layer(x, scope='trans_'+str(i))\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x = self.dense_block(input_x=x, nb_layers=6, layer_name='dense_1')\n",
    "        x = self.transition_layer(x, scope='trans_1')\n",
    "\n",
    "        x = self.dense_block(input_x=x, nb_layers=12, layer_name='dense_2')\n",
    "        x = self.transition_layer(x, scope='trans_2')\n",
    "\n",
    "        x = self.dense_block(input_x=x, nb_layers=48, layer_name='dense_3')\n",
    "        x = self.transition_layer(x, scope='trans_3')\n",
    "\n",
    "        x = self.dense_block(input_x=x, nb_layers=32, layer_name='dense_final')\n",
    "\n",
    "\n",
    "\n",
    "        # 100 Layer\n",
    "        x = Batch_Normalization(x, training=self.training, scope='linear_batch')\n",
    "        x = Relu(x)\n",
    "        x = Global_Average_Pooling(x)\n",
    "        x = flatten(x)\n",
    "        x = Linear(x)\n",
    "\n",
    "\n",
    "        # x = tf.reshape(x, [-1, 10])\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "train_x, train_y, test_x, test_y = prepare_data()\n",
    "train_x, test_x = color_preprocessing(train_x, test_x)\n",
    "\n",
    "# image_size = 32, img_channels = 3, class_num = 10 in cifar10\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\n",
    "label = tf.placeholder(tf.float32, shape=[None, class_num])\n",
    "\n",
    "training_flag = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "logits = DenseNet(x=x, nb_blocks=nb_block, filters=growth_k, training=training_flag).model\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))\n",
    "\n",
    "\"\"\"\n",
    "l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=nesterov_momentum, use_nesterov=True)\n",
    "train = optimizer.minimize(cost + l2_loss * weight_decay)\n",
    "In paper, use MomentumOptimizer\n",
    "init_learning_rate = 0.1\n",
    "but, I'll use AdamOptimizer\n",
    "\"\"\"\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state('./model')\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\n",
    "    epoch_learning_rate = init_learning_rate\n",
    "    for epoch in range(1, total_epochs + 1):\n",
    "        if epoch == (total_epochs * 0.5) or epoch == (total_epochs * 0.75):\n",
    "            epoch_learning_rate = epoch_learning_rate / 10\n",
    "\n",
    "        pre_index = 0\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "\n",
    "        for step in range(1, iteration + 1):\n",
    "            if pre_index+batch_size < 50000 :\n",
    "                batch_x = train_x[pre_index : pre_index+batch_size]\n",
    "                batch_y = train_y[pre_index : pre_index+batch_size]\n",
    "            else :\n",
    "                batch_x = train_x[pre_index : ]\n",
    "                batch_y = train_y[pre_index : ]\n",
    "\n",
    "            batch_x = data_augmentation(batch_x)\n",
    "\n",
    "            train_feed_dict = {\n",
    "                x: batch_x,\n",
    "                label: batch_y,\n",
    "                learning_rate: epoch_learning_rate,\n",
    "                training_flag : True\n",
    "            }\n",
    "\n",
    "            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n",
    "            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n",
    "\n",
    "            train_loss += batch_loss\n",
    "            train_acc += batch_acc\n",
    "            pre_index += batch_size\n",
    "\n",
    "            if step == iteration :\n",
    "                train_loss /= iteration # average loss\n",
    "                train_acc /= iteration # average accuracy\n",
    "\n",
    "                train_summary = tf.Summary(value=[tf.Summary.Value(tag='train_loss', simple_value=train_loss),\n",
    "                                                  tf.Summary.Value(tag='train_accuracy', simple_value=train_acc)])\n",
    "\n",
    "                test_acc, test_loss, test_summary = Evaluate(sess)\n",
    "\n",
    "                summary_writer.add_summary(summary=train_summary, global_step=epoch)\n",
    "                summary_writer.add_summary(summary=test_summary, global_step=epoch)\n",
    "                summary_writer.flush()\n",
    "\n",
    "                line = \"epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n\" % (\n",
    "                    epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n",
    "                print(line)\n",
    "\n",
    "                with open('logs.txt', 'a') as f :\n",
    "                    f.write(line)\n",
    "\n",
    "\n",
    "\n",
    "        saver.save(sess=sess, save_path='./model/dense.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
